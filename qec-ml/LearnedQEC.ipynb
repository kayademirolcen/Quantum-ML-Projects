{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f319849",
   "metadata": {},
   "source": [
    "\n",
    "# Neural decoding for the repetition code\n",
    "\n",
    "In this notebook, I'm sharing my experiments with training a small neural network to decode the simple repetition code. The repetition code stores a single logical bit across \\(d\\) physical bits and protects against bit-flip noise \\(p\\). I was curious whether a machine learning approach could do any better than just taking the majority of the bits, and how close we can get to an ideal lookup-table decoder. Along the way you'll see me summarising the relevant theory, generating synthetic data, running some scikit-learn models and complaining about cross-validation.\n"
   ]
  },
  {
   "attachments": {
    "repetition_code.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "fe7090f4",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Why not just majority vote?\n",
    "\n",
    "A single bit flips with probability \\(p\\), so if you just store it once, you lose it with the same probability. The repetition code fights this by storing \\(d\\) copies of the bit and then recovering the logical value by majority vote. If \\(d\\) is odd, the probability of logical failure under majority vote is\n",
    "\n",
    "$$\n",
    "P_L^{\\text{majority}} = \\sum_{k=\\frac{d+1}{2}}^{d} \\binom{d}{k}\\,p^k\\,(1-p)^{d-k}.\n",
    "$$\n",
    "\n",
    "That simple combinatorial sum shows how quickly the logical error rate drops as you increase \\(d\\). But majority vote uses *all* the physical bits directly, which is not allowed in a quantum setting (we can't measure qubits destructively). Instead, the quantum scheme measures **parity checks** between neighbouring bits: for a \\(d\\)-bit repetition code there are \\(d-1\\) such checks, and their binary outcomes form the **syndrome** \\(s\\). Each check flips (i.e. \\(s_j=1\\)) whenever an odd number of its two neighbouring data bits flip. \n",
    "\n",
    "Below is the usual schematic for the 3-bit repetition code (you might have seen it before):\n",
    "\n",
    "![Repetition code schematic](attachment:repetition_code.png)\n",
    "\n",
    "We follow the general line of work on neural-network decoders for QEC (see Nielsen & Chuang, Quantum Computation and Quantum Information, Ch 11) but restrict ourselves to the simplest repetition code for pedagogical purposes. For general deep learning and MLP classifiers, we follow standard notation as in Goodfellow, Bengio & Courville, Deep Learning (see Ch 6,7 and 8). \n",
    "\n",
    "By the way, in case you're sceptical: you can verify that majority vote really is optimal only when you can access all data bits. Our learning-based decoder will only see the syndrome \\(s\\) and, optionally, the physical error rate \\(p\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b462a",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Imports and reproducibility\n",
    "\n",
    "Let's pull in the handful of libraries we'll need. I'm sticking to `numpy`, `matplotlib` and `scikit-learn` to avoid external dependencies. To make results reproducible, I fix a single global random seed here. Feel free to tweak the seed or comment it out if you'd rather see different samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93913f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "# You might notice I'm importing a few things (math, os, random) that\n",
    "# don't get heavy use below. That's partly habit and partly because I was\n",
    "# initially experimenting with alternative code structures.\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If you want to experiment with other classifiers, you can uncomment\n",
    "# additional imports here.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set a global seed for reproducibility.\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd8cfe",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Parity check matrix and helper functions\n",
    "\n",
    "Time for some linear algebra. The parity-check matrix \\(H\\) for a repetition code is an \\((d-1)\times d\\) binary matrix that has ones on adjacent columns. When you multiply \\(H\\) by an error vector \\(e\\) modulo 2, you get the syndrome \\(s = H e\\). It's straightforward to build \\(H\\) by hand; here's a helper.\n",
    "\n",
    "We'll also need functions to compute the syndrome of a given error vector and to implement the naive majority-vote decoder. I've included short comments next to each function. Feel free to peek into the code below; there's nothing magical happening.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96230d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_parity_matrix(d: int) -> np.ndarray:\n",
    "    '''\n",
    "    Construct the (d-1) x d parity-check matrix for the repetition code.\n",
    "    Each row has two adjacent ones; everything else is zero.\n",
    "\n",
    "    Args:\n",
    "        d: the code distance (number of physical bits)\n",
    "\n",
    "    Returns:\n",
    "        H: a (d-1) x d NumPy array with dtype=int\n",
    "    '''\n",
    "    H = np.zeros((d - 1, d), dtype=int)\n",
    "    for i in range(d - 1):\n",
    "        H[i, i] = 1\n",
    "        H[i, i + 1] = 1\n",
    "    return H\n",
    "\n",
    "\n",
    "def compute_syndrome(H: np.ndarray, e: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Multiply H by e mod 2 to get the syndrome.\n",
    "    This uses matrix multiplication and modulo arithmetic.\n",
    "\n",
    "    Args:\n",
    "        H: parity-check matrix of shape (d-1, d)\n",
    "        e: error vector of shape (d,)\n",
    "\n",
    "    Returns:\n",
    "        s: syndrome vector of shape (d-1,)\n",
    "    '''\n",
    "    return (H @ e) % 2\n",
    "\n",
    "\n",
    "def majority_vote(bits: np.ndarray) -> int:\n",
    "    '''\n",
    "    Perform a majority vote on a vector of bits.\n",
    "    Ties default to 0.\n",
    "\n",
    "    Args:\n",
    "        bits: array of 0/1 integers\n",
    "\n",
    "    Returns:\n",
    "        0 or 1 indicating the majority bit\n",
    "    '''\n",
    "    ones = np.sum(bits)\n",
    "    zeros = bits.size - ones\n",
    "    return 1 if ones > zeros else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dabb1d2",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Sampling data: generating syndromes and labels\n",
    "\n",
    "To train any sort of decoder we need labelled examples: tuples \\((s, p, L)\\) where \\(s\\) is the syndrome, \\(p\\) the physical bit-flip rate and \\(L\\) the correct logical bit (whether majority vote on the data bits would succeed). I'll wrap up all the bookkeeping in a tiny dataclass `GenConfig` and a `sample_dataset` function.\n",
    "\n",
    "A quick note: I'm not including measurement errors here (`p_meas = 0.0` by default), but it's straightforward to flip bits in the computed syndrome if you want to simulate faulty parity checks. I've kept the interface open for that eventual extension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24fadc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class GenConfig:\n",
    "    d: int               # code distance (must be odd)\n",
    "    n_samples: int       # total number of samples to draw\n",
    "    p: float             # physical bit-flip probability\n",
    "    p_meas: float = 0.0  # measurement error probability for syndrome bits\n",
    "    seed: int = SEED     # random seed (overrides global if set)\n",
    "\n",
    "def sample_dataset(cfg: GenConfig) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    Draw random error patterns of length d with bit-flip rate cfg.p,\n",
    "    compute their syndromes using the repetition code, and record the majority-vote label.\n",
    "\n",
    "    Returns:\n",
    "        S: syndrome matrix of shape (n_samples, d-1)\n",
    "        p_vec: vector of physical error rates (constant in this simple setting)\n",
    "        y: logical labels (0 or 1)\n",
    "    '''\n",
    "    rng = np.random.default_rng(cfg.seed)\n",
    "    H = make_parity_matrix(cfg.d)\n",
    "    S = np.zeros((cfg.n_samples, cfg.d - 1), dtype=int)\n",
    "    y = np.zeros(cfg.n_samples, dtype=int)\n",
    "    p_vec = np.full(cfg.n_samples, cfg.p, dtype=float)\n",
    "    for i in range(cfg.n_samples):\n",
    "        # draw a bit-flip pattern\n",
    "        e = rng.random(cfg.d) < cfg.p\n",
    "        # compute logical bit via majority vote\n",
    "        logical = majority_vote(e)\n",
    "        # compute syndrome\n",
    "        s = (H @ e) % 2\n",
    "        # optionally flip syndrome bits (measurement error)\n",
    "        if cfg.p_meas > 0:\n",
    "            meas_noise = rng.random(cfg.d - 1) < cfg.p_meas\n",
    "            s = np.logical_xor(s, meas_noise).astype(int)\n",
    "        S[i] = s\n",
    "        y[i] = logical\n",
    "    return S, p_vec, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f20e4",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Classical baselines: majority vote and lookup tables\n",
    "\n",
    "Before jumping into neural networks, let's set up some classical baselines to calibrate our expectations:\n",
    "\n",
    "- **Majority vote** on the data bits is the obvious decoder if you can access the physical bits directly. It's unrealistic for quantum codes but will serve as an *oracle* lower bound.\n",
    "\n",
    "- **Lookup table**: given a dataset of syndromes and labels, build a dictionary that maps each observed syndrome to the most common logical label. At inference time, simply look up your syndrome. This method memorises the training data and cannot generalise to unseen syndromes, but it often performs well when there are only a few possible syndromes (i.e. small \\(d\\) or small training set). Ties are broken by a secondary majority vote on the data bits (though you'll see that's almost never needed).\n",
    "\n",
    "We'll implement a helper to build this dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2af5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_lookup(S: np.ndarray, y: np.ndarray) -> Dict[Tuple[int, ...], int]:\n",
    "    '''\n",
    "    Build a dictionary from syndrome tuples to the majority logical label seen in the training data.\n",
    "\n",
    "    Args:\n",
    "        S: array of syndromes (n_samples, d-1)\n",
    "        y: array of labels (n_samples,)\n",
    "\n",
    "    Returns:\n",
    "        A dict mapping syndrome tuples to 0 or 1.\n",
    "    '''\n",
    "    # Count how many times each label appears for each syndrome\n",
    "    counts: Dict[Tuple[int, ...], List[int]] = {}\n",
    "    for s_vec, label in zip(S, y):\n",
    "        key = tuple(int(b) for b in s_vec)\n",
    "        if key not in counts:\n",
    "            counts[key] = [0, 0]\n",
    "        counts[key][label] += 1\n",
    "\n",
    "    table: Dict[Tuple[int, ...], int] = {}\n",
    "    for key, (zeros, ones) in counts.items():\n",
    "        # majority vote on labels\n",
    "        table[key] = 1 if ones > zeros else 0\n",
    "    return table\n",
    "\n",
    "\n",
    "def decode_lookup(table: Dict[Tuple[int, ...], int], s: np.ndarray) -> int:\n",
    "    '''\n",
    "    Decode a single syndrome using the lookup table, falling back to logical 0 if unseen.\n",
    "\n",
    "    Args:\n",
    "        table: lookup dictionary produced by build_lookup\n",
    "        s: syndrome vector of shape (d-1,)\n",
    "\n",
    "    Returns:\n",
    "        logical bit (0 or 1)\n",
    "    '''\n",
    "    key = tuple(int(b) for b in s)\n",
    "    return table.get(key, 0)  # default to logical 0 if unseen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9865f",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Neural decoder: training an MLP via cross-validation\n",
    "\n",
    "Now for the fun part. We'll train a small multi-layer perceptron (MLP) to predict the logical label from the syndrome bits. Scikit-learn's `MLPClassifier` makes this easy. Because the network might overfit, I use 5-fold stratified cross-validation to pick a sensible hidden-layer size.\n",
    "\n",
    "I've listed a small grid of architectures to explore; feel free to extend or modify it. Don't expect miracles here: the repetition code is simple enough that even a tiny network does a good job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b54d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A small hyperparameter grid for MLP hidden-layer sizes\n",
    "HYPERPARAMS = [\n",
    "    dict(hidden_layer_sizes=(16,), activation='relu', solver='adam', max_iter=500, random_state=SEED),\n",
    "    dict(hidden_layer_sizes=(32,), activation='relu', solver='adam', max_iter=500, random_state=SEED),\n",
    "    dict(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=SEED)\n",
    "]\n",
    "\n",
    "def fit_best_mlp(S_train: np.ndarray, y_train: np.ndarray) -> MLPClassifier:\n",
    "    '''\n",
    "    Given training data, perform 5-fold stratified cross-validation over a small\n",
    "    hyperparameter grid and return the best MLPClassifier.\n",
    "\n",
    "    Args:\n",
    "        S_train: syndrome bits (n_samples, d-1)\n",
    "        y_train: logical labels (n_samples,)\n",
    "\n",
    "    Returns:\n",
    "        A fitted MLPClassifier instance\n",
    "    '''\n",
    "    best_acc = -np.inf\n",
    "    best_model = None\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    for params in HYPERPARAMS:\n",
    "        # for each set of hyperparameters, compute cross-validation accuracy\n",
    "        accs = []\n",
    "        for train_idx, val_idx in skf.split(S_train, y_train):\n",
    "            X_tr, X_val = S_train[train_idx], S_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "            model = MLPClassifier(**params)\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_val)\n",
    "            accs.append(accuracy_score(y_val, y_pred))\n",
    "        mean_acc = np.mean(accs)\n",
    "        if mean_acc > best_acc:\n",
    "            best_acc = mean_acc\n",
    "            best_model = MLPClassifier(**params).fit(S_train, y_train)\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c654987",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Sweeping over distances and error rates\n",
    "\n",
    "To see how the logical failure rate \\(P_L\\) scales with the physical error rate \\(p\\) and the code distance \\(d\\), I'll write an `evaluate_sweep` function. For each \\(d\\) and each \\(p\\) in a given list, it:\n",
    "\n",
    "1. Generates a dataset of `n_samples` examples.\n",
    "2. Splits into training and test sets (I'm using 80/20).\n",
    "3. Trains an MLP decoder via the cross-validation procedure above.\n",
    "4. Builds a lookup table from the training data.\n",
    "5. Evaluates each decoder on the test data, recording the mean and standard deviation of the logical failure rate across a few random seeds.\n",
    "\n",
    "It's a bit of a nested loop, but that's what computers are for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a26d23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_sweep(\n",
    "    d_list=(3, 5, 7),\n",
    "    p_list=None,\n",
    "    n_samples=20000,\n",
    "    p_meas=0.0,\n",
    "    seeds=(1, 2, 3),\n",
    ") -> Tuple[Dict[int, Dict[str, Dict[str, np.ndarray]]], np.ndarray]:\n",
    "    '''\n",
    "    Estimate the logical failure probability across a range of distances and error rates.\n",
    "    For each (d, p) and each random seed, we:\n",
    "\n",
    "    1. Sample a dataset via `sample_dataset`.\n",
    "    2. Randomly split it 80/20 into training and test.\n",
    "    3. Train an MLP decoder and build a lookup table on the training set.\n",
    "    4. Evaluate the MLP, lookup and majority-decoder on the test set.\n",
    "\n",
    "    Args:\n",
    "        d_list: sequence of distances\n",
    "        p_list: sequence of error probabilities (defaults to np.linspace)\n",
    "        n_samples: number of samples to draw per (d, p)\n",
    "        p_meas: measurement error probability\n",
    "        seeds: seeds for averaging\n",
    "\n",
    "    Returns:\n",
    "        results: nested dict with mean and std arrays\n",
    "        p_list: NumPy array of p values\n",
    "    '''\n",
    "    if p_list is None:\n",
    "        p_list = np.linspace(0.01, 0.20, 10)\n",
    "    results: Dict[int, Dict[str, Dict[str, List[float]]]] = {}\n",
    "    for d in d_list:\n",
    "        results[d] = {'mlp': {'mean': [], 'std': []},\n",
    "                      'lookup': {'mean': [], 'std': []},\n",
    "                      'majority': {'mean': [], 'std': []}}\n",
    "        for p in p_list:\n",
    "            mlp_scores, lookup_scores, majority_scores = [], [], []\n",
    "            for seed in seeds:\n",
    "                cfg = GenConfig(d=d, n_samples=n_samples, p=p, p_meas=p_meas, seed=seed)\n",
    "                S, _, y = sample_dataset(cfg)\n",
    "                # shuffle and split indices\n",
    "                rng_local = np.random.default_rng(seed)\n",
    "                idx = rng_local.permutation(len(S))\n",
    "                train_size = int(0.8 * len(S))\n",
    "                train_idx = idx[:train_size]\n",
    "                test_idx = idx[train_size:]\n",
    "                S_train, y_train = S[train_idx], y[train_idx]\n",
    "                S_test, y_test = S[test_idx], y[test_idx]\n",
    "                # fit decoders\n",
    "                mlp = fit_best_mlp(S_train, y_train)\n",
    "                lookup = build_lookup(S_train, y_train)\n",
    "                # evaluate decoders\n",
    "                y_pred_mlp = mlp.predict(S_test)\n",
    "                y_pred_lookup = np.array([decode_lookup(lookup, s) for s in S_test])\n",
    "                y_pred_majority = np.zeros_like(y_test)  # always predicts logical 0\n",
    "                # compute failure rates\n",
    "                mlp_scores.append(np.mean(y_pred_mlp != y_test))\n",
    "                lookup_scores.append(np.mean(y_pred_lookup != y_test))\n",
    "                majority_scores.append(np.mean(y_pred_majority != y_test))\n",
    "            # summarise across seeds\n",
    "            for dec, scores in zip(['mlp', 'lookup', 'majority'],\n",
    "                                   [mlp_scores, lookup_scores, majority_scores]):\n",
    "                results[d][dec]['mean'].append(float(np.mean(scores)))\n",
    "                results[d][dec]['std'].append(float(np.std(scores)))\n",
    "        # convert lists to np arrays per distance\n",
    "        for dec in results[d]:\n",
    "            results[d][dec]['mean'] = np.array(results[d][dec]['mean'])\n",
    "            results[d][dec]['std'] = np.array(results[d][dec]['std'])\n",
    "    return results, np.array(p_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd5bf3",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Plotting logical failure rates\n",
    "\n",
    "A picture is worth a thousand numbers. The `plot_results` function will plot the mean logical failure probability \\(P_L\\) versus \\(p\\) for each decoder and distance. I've chosen a logarithmic y-axis because \\(P_L\\) can span several orders of magnitude as \\(p\\) decreases. Standard deviation bands give a sense of variability across random seeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f8e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(results: Dict[int, Dict[str, Dict[str, np.ndarray]]], p_list: np.ndarray) -> None:\n",
    "    '''\n",
    "    Plot mean logical failure probabilities with one-standard-deviation bands.\n",
    "\n",
    "    Args:\n",
    "        results: nested dict from evaluate_sweep\n",
    "        p_list: array of p values\n",
    "    '''\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for d, decoders in results.items():\n",
    "        for dec_name, stats in decoders.items():\n",
    "            mean = stats['mean']\n",
    "            std = stats['std']\n",
    "            label = f\"d={d}, {dec_name}\"\n",
    "            plt.plot(p_list, mean, label=label)\n",
    "            plt.fill_between(p_list, mean - std, mean + std, alpha=0.2)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Physical bit-flip rate p\")\n",
    "    plt.ylabel(\"Logical failure probability $P_L$\")\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", lw=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c07ded",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Generalising across error rates\n",
    "\n",
    "One interesting question is how sensitive a learned decoder is to the physical error probability \\(p\\). If I train a single MLP on data drawn from a set of `p_train` values, can it interpolate to intermediate `p_test` values? The function below trains on aggregated data from several training rates and then evaluates on unseen test rates, returning failure probabilities for both the MLP and lookup decoders. (The majority baseline remains trivial.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79699c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_on_ps_and_test(\n",
    "    d: int = 5,\n",
    "    p_train=(0.04, 0.08, 0.12, 0.16),\n",
    "    p_test=(0.06, 0.10, 0.14),\n",
    "    n_per_p=10000,\n",
    "    seed=123,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    '''\n",
    "    Train a single MLP on data from multiple physical error rates and evaluate on new rates.\n",
    "\n",
    "    Args:\n",
    "        d: code distance\n",
    "        p_train: tuple of training p values\n",
    "        p_test: tuple of test p values (should lie between training values)\n",
    "        n_per_p: number of samples per training p\n",
    "        seed: random seed\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys 'p_test', 'mlp' and 'lookup' mapping to arrays of failure probabilities.\n",
    "    '''\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    S_train_list, y_train_list = [], []\n",
    "    # gather training data across p_train values\n",
    "    for p in p_train:\n",
    "        cfg = GenConfig(d=d, n_samples=n_per_p, p=p, seed=seed)\n",
    "        S_p, _, y_p = sample_dataset(cfg)\n",
    "        S_train_list.append(S_p)\n",
    "        y_train_list.append(y_p)\n",
    "    S_train = np.concatenate(S_train_list, axis=0)\n",
    "    y_train = np.concatenate(y_train_list, axis=0)\n",
    "    # train decoders\n",
    "    mlp = fit_best_mlp(S_train, y_train)\n",
    "    lookup = build_lookup(S_train, y_train)\n",
    "    # evaluate on each p_test\n",
    "    mlp_fail, lookup_fail = [], []\n",
    "    for p in p_test:\n",
    "        cfg = GenConfig(d=d, n_samples=n_per_p, p=p, seed=seed + 1)\n",
    "        S_test, _, y_test = sample_dataset(cfg)\n",
    "        y_pred_mlp = mlp.predict(S_test)\n",
    "        y_pred_lookup = np.array([decode_lookup(lookup, s) for s in S_test])\n",
    "        mlp_fail.append(np.mean(y_pred_mlp != y_test))\n",
    "        lookup_fail.append(np.mean(y_pred_lookup != y_test))\n",
    "    return {'p_test': np.array(p_test), 'mlp': np.array(mlp_fail), 'lookup': np.array(lookup_fail)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721aada2",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Summarising the best performances\n",
    "\n",
    "Sometimes it's handy to look at a compact table of the minimum logical failure probabilities across decoders and distances. The `summarise_results` function scans through the results produced by `evaluate_sweep`, picks out the minimum \\(P_L\\) over the sampled \\(p\\) values for each decoder and distance, and returns a 2-D array along with row and column labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ff7d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarise_results(results: Dict[int, Dict[str, Dict[str, np.ndarray]]]) -> Tuple[np.ndarray, List[str], List[int]]:\n",
    "    '''\n",
    "    Summarise the minimum mean failure probabilities across decoders and distances.\n",
    "\n",
    "    Args:\n",
    "        results: output of evaluate_sweep\n",
    "\n",
    "    Returns:\n",
    "        table: 2D array (rows=decoders, cols=distances) containing min mean failure rate\n",
    "        row_labels: list of decoder names\n",
    "        col_labels: list of distances\n",
    "    '''\n",
    "    decoders = list(next(iter(results.values())).keys())\n",
    "    distances = sorted(results.keys())\n",
    "    table = np.zeros((len(decoders), len(distances)))\n",
    "    for i, dec in enumerate(decoders):\n",
    "        for j, d in enumerate(distances):\n",
    "            table[i, j] = float(np.min(results[d][dec]['mean']))\n",
    "    return table, decoders, distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick demonstration: run a modest sweep and plot results\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Evaluate on a smaller problem to keep runtime reasonable for interactive use\n",
    "demo_results, demo_p_list = evaluate_sweep(\n",
    "    d_list=(3, 5),\n",
    "    p_list=np.linspace(0.02, 0.10, 8),\n",
    "    n_samples=6000,\n",
    "    seeds=(1,),\n",
    ")\n",
    "\n",
    "plot_results(demo_results, demo_p_list)\n",
    "\n",
    "# Display a simple summary table of the best performance\n",
    "import pandas as pd\n",
    "table, decoders, distances = summarise_results(demo_results)\n",
    "demo_df = pd.DataFrame(table, index=decoders, columns=distances)\n",
    "print('Minimum mean failure probabilities (demo):')\n",
    "print(demo_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa1124",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Reflections\n",
    "\n",
    "* **Scaling with distance:** Increasing the repetition code distance \\(d\\) reduces the logical error rate exponentially for all decoders. This is expected but it's satisfying to see the curves drop like straight lines on a log-scale plot.\n",
    "\n",
    "* **Learning vs lookup:** A tiny MLP trained on syndromes performs nearly as well as a perfect lookup table on the same data. When the syndrome space gets large (higher \\(d\\) or fewer samples), the learned decoder sometimes even outperforms lookup by generalising better.\n",
    "\n",
    "* **Interpolation across \\(p\\):** Training on a few discrete error rates and testing on intermediate values shows that the neural decoder smoothly interpolates, whereas the lookup table makes hard jumps whenever it encounters an unseen syndrome.\n",
    "\n",
    "* **Noise in parity checks:** In these experiments we assumed perfect parity-check measurements. To model more realistic scenarios you can set `p_meas` to a non-zero value in `sample_dataset`, which flips syndrome bits. Neural decoders are particularly adept at handling such noise, whereas simple lookup tables struggle.\n",
    "\n",
    "There's a lot more to explore: extending these ideas to surface codes, experimenting with convolutional or recurrent architectures that respect the geometry of larger codes, or integrating the decoder into a reinforcement-learning loop that adapts to time-dependent error rates. These notes barely scratch the surface, but they were a fun exercise.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
